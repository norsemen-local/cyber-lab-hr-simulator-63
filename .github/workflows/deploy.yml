name: HR Portal AWS Deployment
on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        default: 'deploy'
        type: choice
        options:
          - deploy
          - destroy
      confirmation:
        description: 'Type "destroy" to confirm deletion of all resources'
        required: false
        type: string
      force_deploy:
        description: 'Force deployment even if resources exist'
        required: false
        default: 'false'
        type: boolean

jobs:
  # Check Environment Stage
  check_environment:
    runs-on: ubuntu-latest
    outputs:
      resources_exist: ${{ steps.check_resources.outputs.resources_exist }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Check for existing resources
        id: check_resources
        run: |
          # Check for existing EC2 instance
          EC2_INSTANCES=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=hr-portal-ec2" "Name=instance-state-name,Values=pending,running,stopping,stopped" --query "Reservations[*].Instances[*].InstanceId" --output text)

          # Check for existing ALB
          ALB_EXISTS=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, 'hr-portal')].LoadBalancerArn" --output text)

          # Check for existing API Gateway
          API_EXISTS=$(aws apigateway get-rest-apis --query "items[?contains(name, 'hr-portal')].id" --output text | awk '{print $1}' )

          # Store resource details for later use if they exist
          if [ -n "$EC2_INSTANCES" ]; then
            echo "EC2 instances found: $EC2_INSTANCES"
            # Get details of first instance for reference
            INSTANCE_ID=$(echo $EC2_INSTANCES | awk '{print $1}')
            INSTANCE_DETAILS=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID --output json)

            # Extract important details
            VPC_ID=$(echo "$INSTANCE_DETAILS" | jq -r '.Reservations[0].Instances[0].VpcId')
            SUBNET_ID=$(echo "$INSTANCE_DETAILS" | jq -r '.Reservations[0].Instances[0].SubnetId')
            SECURITY_GROUPS=$(echo "$INSTANCE_DETAILS" | jq -r '.Reservations[0].Instances[0].SecurityGroups[].GroupId' | tr '\n' ' ')
            IAM_PROFILE=$(echo "$INSTANCE_DETAILS" | jq -r '.Reservations[0].Instances[0].IamInstanceProfile.Arn // "none"')
            AMI_ID=$(echo "$INSTANCE_DETAILS" | jq -r '.Reservations[0].Instances[0].ImageId')

            echo "vpc_id=$VPC_ID" >> $GITHUB_ENV
            echo "subnet_id=$SUBNET_ID" >> $GITHUB_ENV
            echo "security_groups=$SECURITY_GROUPS" >> $GITHUB_ENV
            echo "iam_profile=$IAM_PROFILE" >> $GITHUB_ENV
            echo "ami_id=$AMI_ID" >> $GITHUB_ENV
          fi

          if [ -n "$ALB_EXISTS" ]; then
            echo "ALB ARN: $ALB_EXISTS"
            ALB_DETAILS=$(aws elbv2 describe-load-balancers --load-balancer-arns $ALB_EXISTS --output json)
            ALB_DNS=$(echo "$ALB_DETAILS" | jq -r '.LoadBalancers[0].DNSName')
            echo "alb_dns=$ALB_DNS" >> $GITHUB_ENV

            # Get target groups
            TARGET_GROUPS=$(aws elbv2 describe-target-groups --load-balancer-arn $ALB_EXISTS --output json)
            TARGET_GROUP_ARN=$(echo "$TARGET_GROUPS" | jq -r '.TargetGroups[0].TargetGroupArn // "none"')
            echo "target_group_arn=$TARGET_GROUP_ARN" >> $GITHUB_ENV
          fi

          if [ -n "$API_EXISTS" ]; then
            echo "API Gateway ID: $API_EXISTS"
            API_DETAILS=$(aws apigateway get-rest-api --rest-api-id $API_EXISTS --output json)
            API_URL="https://${API_EXISTS}.execute-api.${AWS_REGION}.amazonaws.com/prod"
            echo "api_url=$API_URL" >> $GITHUB_ENV
          fi

          if [ -n "$EC2_INSTANCES" ] || [ -n "$ALB_EXISTS" ] || [ -n "$API_EXISTS" ]; then
            echo "AWS resources for HR Portal already exist."
            echo "resources_exist=true" >> $GITHUB_OUTPUT
          else
            echo "No existing HR Portal resources found."
            echo "resources_exist=false" >> $GITHUB_OUTPUT
          fi

  # Deploy Infrastructure Stage
  deploy_resources:
    needs: [check_environment]
    if: ${{ github.event.inputs.action != 'destroy' }}
    runs-on: ubuntu-latest
    outputs:
      ec2_ip: ${{ steps.terraform_outputs.outputs.EC2_IP }}
      api_url: ${{ steps.terraform_outputs.outputs.API_URL }}
      alb_dns: ${{ steps.terraform_outputs.outputs.ALB_DNS }}
      instance_id: ${{ steps.terraform_outputs.outputs.INSTANCE_ID }}
    env:
      REBUILD_EC2_ONLY: ${{ needs.check_environment.outputs.resources_exist == 'true' && github.event.inputs.force_deploy == 'true' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2

      - name: Terraform Init & Apply
        run: |
          if [ "${{ needs.check_environment.outputs.resources_exist }}" == "true" ]; then
            echo "Resources already exist. Using update-ec2-only.sh to avoid recreating existing infrastructure..."
            chmod +x ./update-ec2-only.sh
            ./update-ec2-only.sh us-east-1
          else
            echo "Deploying full infrastructure..."
            cd terraform
            terraform init
            terraform plan -var="db_username=admin" -var="db_password=password123"
            terraform apply -auto-approve -var="db_username=admin" -var="db_password=password123"
          fi

      - name: Get infrastructure outputs
        id: terraform_outputs
        run: |
          # Set default values
          EC2_IP="unavailable"
          ALB_DNS="unavailable"
          API_URL="unavailable"
          INSTANCE_ID="unavailable"

          if [ "$REBUILD_EC2_ONLY" == "true" ]; then
            # Get EC2 details directly from AWS for EC2-only updates
            echo "Getting resource details after EC2-only update..."

            # Add a short delay to make sure AWS has registered the new instance
            echo "Waiting 10 seconds for AWS to register the new instance..."
            sleep 10

            # Get the new instance ID with more specific filter for running instances only
            # Sort by launch time to get the most recently launched instance
            INSTANCE_ID=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=hr-portal-ec2" "Name=instance-state-name,Values=pending,running" \
              --query "sort_by(Reservations[].Instances[], &LaunchTime)[-1].InstanceId" \
              --output text || echo "unavailable")

            echo "Found instance ID: $INSTANCE_ID"

            if [ "$INSTANCE_ID" != "unavailable" ] && [ "$INSTANCE_ID" != "None" ]; then
              # Get the public IP of the new instance
              EC2_IP=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID \
                --query "Reservations[0].Instances[0].PublicIpAddress" \
                --output text || echo "unavailable")

              # Find ALB DNS name
              ALB_DNS=$(aws elbv2 describe-load-balancers \
                --query "LoadBalancers[?contains(LoadBalancerName, 'hr-portal')].DNSName" \
                --output text || echo "unavailable")

              # Find API Gateway URL
              API_ID=$(aws apigateway get-rest-apis \
                --query "items[?contains(name, 'hr-portal')].id" \
                --output text | awk '{print $1}')
              if [ -n "$API_ID" ] && [ "$API_ID" != "None" ]; then
                API_URL="https://${API_ID}.execute-api.${AWS_REGION}.amazonaws.com/prod"
              fi
            fi
          else
            # Regular Terraform-based resource information extraction
            cd terraform

            # Display all terraform outputs for debugging
            echo "Terraform outputs:"
            terraform output || echo "No outputs available"


            
            # If that didn't work, try JSON format as fallback
            if [ "$EC2_IP" == "unavailable" ] && [ "$ALB_DNS" == "unavailable" ] && [ "$API_URL" == "unavailable" ] && [ "$INSTANCE_ID" == "unavailable" ]; then
                # If terraform outputs aren't available, set defaults
                echo "Terraform outputs not available, setting defaults..."
                EC2_IP="unavailable"
                ALB_DNS="unavailable"
                API_URL="unavailable"
                INSTANCE_ID="unavailable"
              
            fi
          fi

          # Handle AWS CLI output parsing errors
          set +e
          
          echo "Values after extraction:"
          echo "EC2_IP: $EC2_IP"
          echo "ALB_DNS: $ALB_DNS" 
          echo "API_URL: $API_URL"
          echo "INSTANCE_ID: $INSTANCE_ID"

          # Check if any values are missing and attempt to get them from AWS CLI
          echo "Checking for missing values and retrieving from AWS CLI if needed..."

          # If instance ID is missing, try to get it
          if [ "$INSTANCE_ID" == "unavailable" ] || [ "$INSTANCE_ID" == "None" ]; then
            echo "Instance ID is missing, fetching from AWS CLI..."
            INSTANCE_ID=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=hr-portal-ec2" "Name=instance-state-name,Values=pending,running" --query "sort_by(Reservations[].Instances[], &LaunchTime)[-1].InstanceId" --output text || echo "unavailable")
            echo "Retrieved Instance ID: $INSTANCE_ID"
          fi

          # If EC2 IP is missing but we have instance ID, get the IP
          if { [ "$EC2_IP" == "unavailable" ] || [ "$EC2_IP" == "None" ]; } && [ "$INSTANCE_ID" != "unavailable" ] && [ "$INSTANCE_ID" != "None" ]; then
            echo "EC2 IP is missing, fetching from AWS CLI using instance ID: $INSTANCE_ID"
            EC2_IP=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID --query "Reservations[0].Instances[0].PublicIpAddress" --output text || echo "unavailable")
            echo "Retrieved EC2 IP: $EC2_IP"
          fi

          # If ALB DNS is missing, try to get it
          if [ "$ALB_DNS" == "unavailable" ] || [ "$ALB_DNS" == "None" ]; then
            echo "ALB DNS is missing, fetching from AWS CLI..."
            ALB_DNS=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, 'hr-portal')].DNSName" --output text || echo "unavailable")
            echo "Retrieved ALB DNS: $ALB_DNS"
          fi

          # If API URL is missing, try to get it
          if [ "$API_URL" == "unavailable" ] || [ "$API_URL" == "None" ]; then
            echo "API URL is missing, fetching from AWS CLI..."
            API_ID=$(aws apigateway get-rest-apis --query "items[?contains(name, 'hr-portal')].id" --output text | awk '{print $1}')
            if [ -n "$API_ID" ] && [ "$API_ID" != "None" ]; then
              API_URL="https://${API_ID}.execute-api.${AWS_REGION}.amazonaws.com/prod"
              echo "Retrieved API URL: $API_URL"
            else
              echo "Could not retrieve API ID from AWS CLI"
            fi
          fi

          echo "EC2_IP=${EC2_IP}" >> $GITHUB_OUTPUT
          echo "API_URL=${API_URL}" >> $GITHUB_OUTPUT
          echo "ALB_DNS=${ALB_DNS}" >> $GITHUB_OUTPUT
          echo "INSTANCE_ID=${INSTANCE_ID}" >> $GITHUB_OUTPUT

          echo "EC2 IP: ${EC2_IP}"
          echo "ALB DNS: ${ALB_DNS}"
          echo "API URL: ${API_URL}"
          echo "Instance ID: ${INSTANCE_ID}"

  # Combined Instance Preparation Stage
  prepare_instance:
    needs: deploy_resources
    if: ${{ needs.deploy_resources.outputs.instance_id != 'unavailable' }}
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Wait for instance to be ready
        run: |
          INSTANCE_ID=${{ needs.deploy_resources.outputs.instance_id }}

          echo "Waiting for instance $INSTANCE_ID to be fully initialized..."

          # Set a reasonable timeout
          timeout=900  # 15 minutes
          interval=15  # 15 seconds
          elapsed=0

          # Wait for instance to be running
          echo "Checking if instance is running..."
          while [ $elapsed -lt $timeout ]; do
            STATUS=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID --query 'Reservations[0].Instances[0].State.Name' --output text || echo "unknown")
            echo "Instance status: $STATUS"

            if [ "$STATUS" == "running" ]; then
              echo "Instance is running!"
              break
            elif [ "$STATUS" == "terminated" ] || [ "$STATUS" == "shutting-down" ]; then
              echo "Instance is in a terminal state: $STATUS"
              echo "Continuing workflow but deployment may fail"
              break
            fi

            sleep $interval
            elapsed=$((elapsed + interval))
            echo "Waited for $elapsed seconds..."
          done

          if [ $elapsed -ge $timeout ]; then
            echo "Timeout waiting for instance to be running, but continuing workflow"
          fi

          # Wait additional time for services to start
          echo "Waiting an additional 2 minutes for all services to start..."
          sleep 120

      - name: Verify and fix SSM agent
        run: |
          INSTANCE_ID=${{ needs.deploy_resources.outputs.instance_id }}

          echo "Verifying SSM agent status on instance $INSTANCE_ID..."

          # Check if instance is already registered with SSM
          SSM_ONLINE=$(aws ssm describe-instance-information --filters "Key=InstanceIds,Values=$INSTANCE_ID" --query "InstanceInformationList[0].PingStatus" --output text 2>/dev/null || echo "ConnectionLost")

          echo "SSM Agent status: $SSM_ONLINE"

          # If SSM is working, we're done
          if [ "$SSM_ONLINE" == "Online" ]; then
            echo "SSM Agent is online and working properly."
            exit 0
          fi

          # Otherwise try to restart the SSM agent using EC2 Instance Connect
          echo "SSM Agent is not online. Attempting recovery..."

          # First try: Use AWS EC2 Instance Connect to run commands directly on the instance
          echo "Attempting to connect to instance using EC2 Instance Connect..."

          # Check if instance is in a running state
          INSTANCE_STATE=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID --query "Reservations[0].Instances[0].State.Name" --output text)

          if [ "$INSTANCE_STATE" != "running" ]; then
            echo "Instance is not in 'running' state (current state: $INSTANCE_STATE). Cannot attempt recovery."
            echo "WARNING: Subsequent deployment steps may fail without SSM connectivity."
            exit 0
          fi

          # Try to use EC2 Instance Connect to run the recovery commands
          # We'll create a temporary SSH key for this purpose
          echo "Generating temporary SSH key for EC2 Instance Connect..."
          ssh-keygen -t rsa -f /tmp/temp_key -N "" -q

          # Get the public key
          PUB_KEY=$(cat /tmp/temp_key.pub)

          # Get the instance's availability zone
          AZ=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID --query "Reservations[0].Instances[0].Placement.AvailabilityZone" --output text)

          # Try to push the public key to the instance
          echo "Pushing SSH key to instance..."
          aws ec2-instance-connect send-ssh-public-key \
            --instance-id $INSTANCE_ID \
            --availability-zone $AZ \
            --instance-os-user ec2-user \
            --ssh-public-key "$PUB_KEY"

          # Sleep for a moment to allow the key to propagate
          sleep 5

          # Get the instance's public IP address
          PUBLIC_IP=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID --query "Reservations[0].Instances[0].PublicIpAddress" --output text)

          echo "Attempting to connect to $PUBLIC_IP via SSH to restart SSM agent..."

          # Use SSH with StrictHostKeyChecking=no to execute commands
          ssh -i /tmp/temp_key -o StrictHostKeyChecking=no -o ConnectTimeout=10 ec2-user@$PUBLIC_IP << 'EOF' || echo "SSH connection failed, but continuing..."
          sudo amazon-linux-extras install -y amazon-ssm-agent
          sudo systemctl enable amazon-ssm-agent
          sudo systemctl restart amazon-ssm-agent
          echo "SSM agent restarted at $(date)"
          EOF

          # Wait for the recovery to take effect
          echo "Waiting for SSM agent recovery (60 seconds)..."
          sleep 60

          # Check again if recovery was successful
          SSM_ONLINE_AFTER=$(aws ssm describe-instance-information --filters "Key=InstanceIds,Values=$INSTANCE_ID" --query "InstanceInformationList[0].PingStatus" --output text 2>/dev/null || echo "ConnectionLost")

          echo "SSM Agent status after recovery attempt: $SSM_ONLINE_AFTER"

      - name: Setup Docker
        run: |
          INSTANCE_ID=${{ needs.deploy_resources.outputs.instance_id }}

          echo "Setting up Docker on instance $INSTANCE_ID..."

          # Use SSM to check Docker installation
          aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --targets "Key=InstanceIds,Values=$INSTANCE_ID" \
            --parameters 'commands=[
              "echo \"===== DOCKER VERIFICATION =====\"",
              "# Check if Docker is installed",
              "if ! command -v docker &> /dev/null; then",
              "  echo \"Docker is not installed. Installing...\"",
              "  amazon-linux-extras install -y docker || yum install -y docker",
              "  systemctl enable docker",
              "  systemctl start docker",
              "  echo \"Docker installation complete.\"",
              "else",
              "  echo \"Docker is already installed.\"",
              "fi",

              "# Check if Docker service is running",
              "if ! systemctl is-active --quiet docker; then",
              "  echo \"Docker service is not running. Starting...\"",
              "  systemctl start docker",
              "  echo \"Docker service started.\"",
              "else",
              "  echo \"Docker service is running.\"",
              "fi",

              "# Verify Docker is working",
              "echo \"Testing Docker with hello-world container...\"",
              "docker run --rm hello-world | grep \"Hello from Docker!\" && echo \"Docker is working correctly.\" || echo \"Docker test failed.\"",

              "# Create application directory",
              "echo \"Creating application directory...\"",
              "mkdir -p /tmp/hrApp",
              "chmod -R 777 /tmp/hrApp",
              "mkdir -p /opt/hrApp",
              "chmod -R 777 /opt/hrApp",
              "echo \"Directory prepared at $(date)\" > /tmp/hrApp/prepared.txt",
              "echo \"Directory prepared at $(date)\" > /opt/hrApp/prepared.txt",
              "echo \"===== VERIFICATION COMPLETE =====\""
            ]' \
            --comment "Verify Docker installation"

          # Allow time for command to complete
          echo "Waiting for Docker setup to complete (30 seconds)..."
          sleep 30

  # Deploy Application Stage
  deploy_application:
    needs: [deploy_resources, prepare_instance]
    if: ${{ needs.deploy_resources.outputs.instance_id != 'unavailable' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Build Docker image
        run: |
          echo "Building Docker image..."
          docker build -t hr-portal-app:latest .
          docker save hr-portal-app:latest > hr-portal-app.tar

      - name: Create S3 bucket for Docker image
        id: create_bucket
        run: |
          # Create unique bucket name
          BUCKET_NAME="hr-portal-docker-temp-$(date +%s)"
          echo "bucket_name=$BUCKET_NAME" >> $GITHUB_OUTPUT

          # Create temporary S3 bucket with unique name 
          echo "Creating temporary S3 bucket: $BUCKET_NAME"
          aws s3 mb s3://$BUCKET_NAME

          # Set bucket policy
          cat > /tmp/bucket-policy.json <<EOL
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": {
                  "AWS": "arn:aws:iam::*:role/hr-portal-ec2-role"
                },
                "Action": ["s3:GetObject", "s3:ListBucket"],
                "Resource": [
                  "arn:aws:s3:::$BUCKET_NAME",
                  "arn:aws:s3:::$BUCKET_NAME/*"
                ]
              }
            ]
          }
          EOL

          aws s3api put-bucket-policy --bucket $BUCKET_NAME --policy file:///tmp/bucket-policy.json || echo "Could not set bucket policy, continuing..."

          # Upload Docker image to S3
          echo "Uploading Docker image to S3..."
          aws s3 cp hr-portal-app.tar s3://$BUCKET_NAME/

          echo "Bucket URL: s3://$BUCKET_NAME"

      - name: Deploy Docker container to EC2
        run: |
          INSTANCE_ID=${{ needs.deploy_resources.outputs.instance_id }}
          BUCKET_NAME=${{ steps.create_bucket.outputs.bucket_name }}

          # Deploy using SSM
          echo "Deploying Docker container to EC2..."
          aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --targets "Key=InstanceIds,Values=$INSTANCE_ID" \
            --parameters "{\"commands\":[
              \"echo \\\"====== STARTING DEPLOYMENT \$(date) ======\\\" > /tmp/deployment.log\",

              \"aws s3 cp s3://$BUCKET_NAME/hr-portal-app.tar /opt/hrApp/ >> /opt/hrApp/deployment.log 2>&1\",

              \"echo \\\"Loading Docker image...\\\" >> /tmp/deployment.log\",
              \"sudo docker load -i /opt/hrApp/hr-portal-app.tar >> /tmp/deployment.log 2>&1\",

              \"echo \\\"Removing existing container if any...\\\" >> /tmp/deployment.log\",
              \"sudo docker stop hr-portal-container >> /tmp/deployment.log 2>&1 || echo \\\"No container to stop\\\" >> /tmp/deployment.log\",
              \"sudo docker rm hr-portal-container >> /tmp/deployment.log 2>&1 || echo \\\"No container to remove\\\" >> /tmp/deployment.log\",

              \"echo \\\"Running new container...\\\" >> /tmp/deployment.log\",
              \"sudo docker run -d --name hr-portal-container -p 80:80 -e API_GATEWAY_ALLOW_ALL=true hr-portal-app:latest >> /tmp/deployment.log 2>&1\",

              \"echo \\\"Verifying container is running...\\\" >> /tmp/deployment.log\",
              \"sudo docker ps >> /tmp/deployment.log 2>&1\",

              \"echo \\\"====== DEPLOYMENT COMPLETED \$(date) ======\\\" >> /tmp/deployment.log\",
              \"cat /tmp/deployment.log\"
            ]}" \
            --comment "Deploy HR Portal Docker container"

      - name: Verify Deployment
        run: |
          INSTANCE_ID=${{ needs.deploy_resources.outputs.instance_id }}

          echo "Waiting for container to start (30 seconds)..."
          sleep 30

          echo "Verifying deployment..."
          aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --targets "Key=InstanceIds,Values=$INSTANCE_ID" \
            --parameters '{"commands":[
              "echo \"===== DEPLOYMENT VERIFICATION =====\"",
              "echo \"Container status:\"",
              "sudo docker ps",
              "echo \"Container logs:\"",
              "sudo docker logs hr-portal-container 2>&1 || echo \"No logs available\"",
              "echo \"===== VERIFICATION COMPLETE =====\""
            ]}' \
            --comment "Verify HR Portal deployment"

  # Display URLs Stage
  display_urls:
    needs: [deploy_resources, deploy_application]
    if: ${{ always() && github.event.inputs.action != 'destroy' }}
    runs-on: ubuntu-latest
    steps:
      - name: Display deployment URLs
        run: |
          echo "============= DEPLOYMENT INFORMATION ============="
          echo "HR Portal has been deployed successfully!"
          echo ""
          echo "Application Load Balancer (ALB) URL:"
          echo "http://${{ needs.deploy_resources.outputs.alb_dns }}"
          echo ""
          echo "API Gateway URL:"
          echo "${{ needs.deploy_resources.outputs.api_url }}"
          echo ""
          echo "EC2 Instance Direct URL (for debugging):"
          echo "http://${{ needs.deploy_resources.outputs.ec2_ip }}"
          echo "=================================================="

  # Destroy Resources Stage
  destroy:
    needs: check_environment
    if: ${{ github.event.inputs.action == 'destroy' && github.event.inputs.confirmation == 'destroy' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2

      - name: Terraform Destroy
        run: |
          cd terraform
          terraform init
          terraform destroy -auto-approve -var="db_username=admin" -var="db_password=password123"

      - name: Confirmation
        run: |
          echo "All AWS resources have been successfully destroyed."
